# ğŸ§  Neural Networks from Scratch

This repository, **Neural_networks**, contains implementations of several fundamental **neural network architectures** built entirely **from scratch** (no high-level deep learning frameworks like PyTorch or TensorFlow) and applied to the **MNIST** handwritten digits dataset.

The goal of this project is to provide educational, transparent, and clean code to understand the inner workings of different types of neural networks.

---

## ğŸ“‚ Repository Structure
Neural_networks/  
â”œâ”€â”€ CNN/ # Convolutional Neural Network  
â”œâ”€â”€ Hebbian/ # Hebbian Learning Rule implementation  
â”œâ”€â”€ Perceptron/ # Single-layer Perceptron  
â”œâ”€â”€ Adaline/ # ADAptive LInear NEuron  
â””â”€â”€ Multilayer_perceptron/ # Fully connected feedforward network (MLP)  

---

## ğŸ“Š Neural Network Types

### ğŸ”¹ Perceptron
- Binary classifier based on a simple threshold activation
- Trained using the perceptron learning rule

### ğŸ”¹ Adaline
- Similar to Perceptron but uses linear activation and minimizes mean squared error

### ğŸ”¹ Hebbian
- Unsupervised learning based on Hebb's rule: â€œcells that fire together wire togetherâ€

### ğŸ”¹ Multilayer Perceptron (MLP)
- Feedforward neural network with one or more hidden layers
- Trained using backpropagation and gradient descent

### ğŸ”¹ Convolutional Neural Network (CNN)
- Designed to process image data efficiently
- Includes convolutional and pooling layers for feature extraction

---

## ğŸ§ª Dataset: MNIST

All models are trained and evaluated on the **MNIST** dataset of 28x28 grayscale handwritten digits (0â€“9). Each network is adapted to handle this format and evaluate classification performance.
